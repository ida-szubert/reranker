\section*{Q2}

Subjectively, the translations chosen with reversed language model weight are not obviously worse, or even less fluent. They do, however seem to be longer. Since the longer the sentence the smaller the language model probability it is to be expected that minimizing this feature value will favour longer outputs. For instance, the first example in table~\ref{translation_compare} shows sentences which arguably do not differ in content, although the second uses 10 words for what the first expresses in 3..

It seems that sometimes the preference for shortness leads to loss of information, e.g. in the second example. While both are good English, the second one is closer to the reference translation which includes the phrase \textit{"this summer's tragedy"}. In this example by reversing the p(e) weight we actually choose a better candidate. That the default reranker leaves information out is evident from comparison of sentences in the third example. The default translation simply does not include the adjectives modifying \textit{language} and \textit{question}.

In some cases reversing the bias for language model probability leads to choosing more comprehensible translations, as seen in the next example in table~\ref{translation_compare}.

There are examples of LM likelihood value actually tracking what we'd like it to track, namely English fluency and idiomaticity. The last example in this section of table~\ref{translation_compare} shows an idiomatic expression \textit{keep in mind} being dealt with correctly by the default, but not the LM-flipped, reranker.