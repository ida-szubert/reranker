\section*{Q7}

We based our decoder on the correspondence between phrase-based decoding and
the Traveling Salesman Problem, following the proposal
of~\cite{zaslavskiy2009}.

Our implementation includes the following steps:
\begin{enumerate}
    \item project the translation problem to an \emph{Asymmetric Generalized
        Travelling Salesman Problem} [\textsc{AGTSP}].
    \item convert the \textsc{AGTSP} to an \emph{Asymmetric Travelling
        Salesman Problem} [\textsc{ATSP}].
    \item find the best path by utilizing the LKH
        package\footnote{\url{http://www.akira.ruc.dk/~keld/research/LKH/}}
        implementation of the Lin-Kernighan~heuristic~(\cite{Helsgaun2006}).
\end{enumerate}

We transform a sentence into an asymmetric graph by following the procedure
described in~\cite{zaslavskiy2009}: We extract all possible phrases from the
French sentence. For each phrase we retrieve $k$ possible translations and
store each pair as a bi-phrase. For each combination of a French word and a
bi-phrase in which it occurs we create a node (word, bi-phrase) in the
\textsc{AGTSP} graph. Nodes sharing the same French word form a group. In
\textsc{AGTSP}, each group has to be visited once, which means that the
algorithm forces the final translation to cover each French word.
We decide on costs for each directed edge following the approach described in the
article. Edges connecting nodes which represent consecutive words in the one phrase 
carry zero costs, whereas the cost of transitions between phrases is determined by
the translation model, language model, and the distance between the phrases in
the French sentence.

We have to convert our \textsc{AGTSP} to an \textsc{ATSP} so that we can use
the solver by~\cite{Helsgaun2006}. This projection is done in polynomial
time and converts each group to a directed cycle of nodes connected with low-cost
edges. The exact conversion is described in the article.

The original article optimizes three parameters to weigh the relative
importance of translation model, language model, and phrase distance.
We simplified the model by setting these parameters to constant 1. We also
do no follow the article in further transforming the ATPS graph to a STPS one.
Instead we use a solver which implements the same algorithm as the one used 
in the article, but can operate on the directed \textsc{ATSP}s.

The proposed model does not reach the same translation quality as the
default non-swapping decoder, as measured by \texttt{compute-model-score}.
A possible difficulty lies in the fact that our edge costs are calculated in
advance, so that our language model has only a limited context to work on.
This is especially a problem for one-word phrases which consist of
out-of-dictionary words. A second problem might be that the \textsc{ATSP}
solver does not guarantee the optimal solution, although we did not spot any
obviously non-optimal solutions in our sample documents.
Another, most likely cause of poor performance, might be an error in our implementation 
of edge cost calculation. There are many factors which influence what kind of an edge connects 
any two nodes in the graph -- are the nodes in the same cluster, do they share a phrase, is any 
one of them the start-of-sentence-node, etc.
In Figure~\ref{decode3}, we see the corpus log-probability of our decoder for
various numbers of possible translations $k$.

\begin{figure}
	\centering
	\includegraphics[scale=.5]{figures/TSP_k.pdf}
	\caption{Performance of the TSP decoder as a function of number of translations.}\label{decode3}
\end{figure}
